## 优化理论

机器学习训练模型的过程，本质上是在参数空间中寻找使损失函数最小的点,这就是优化问题。

### 基本优化问题

无约束优化：

$$
\min_{\mathbf{x}} f(\mathbf{x})
$$

带约束优化：

$$
\min_{\mathbf{x}} f(\mathbf{x})
\quad
\text{s.t.}
\quad
g_i(\mathbf{x})\le 0
$$

### 梯度下降

梯度定义：

$$
\nabla f(\mathbf{x}) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\\\ \vdots \\\\ \frac{\partial f}{\partial x_n} \end{bmatrix}
$$

更新公式：

$$
\mathbf{x}_{t+1} = \mathbf{x}_t - \eta \nabla f(\mathbf{x}_t)
$$

### 凸函数

定义：

$$
f(\lambda x+(1-\lambda)y) \le \lambda f(x)+(1-\lambda)f(y)
$$

若二阶可导，则：

$$
\nabla^2 f(x) \succeq 0
$$

### 牛顿法

二阶泰勒展开：

$$
f(x+\Delta x) \approx f(x) + \nabla f(x)^T\Delta x + \frac{1}{2}\Delta x^T H(x)\Delta x
$$

更新公式：

$$
x_{t+1} = x_t - H^{-1}(x_t)\nabla f(x_t)
$$

### 随机梯度下降（SGD）

$$
x_{t+1} = x_t - \eta \nabla f_i(x_t)
$$

适用于大规模数据。

### KKT 条件

拉格朗日函数：

$$
\mathcal{L}(x,\lambda) = f(x) + \sum_i \lambda_i g_i(x)
$$

KKT 条件：

1.  梯度为零
2.  原始可行
3.  对偶可行
4.  互补松弛

### 总结

梯度给出下降方向；凸性保证最优解唯一；牛顿法利用曲率；SGD 提升规模效率；KKT 解决约束问题。
