## 多变量微积分

如果说一元微积分教会我们“**变化**”，那么多变量微积分教会我们的就是“**在高维空间里变化**”。这听起来有点抽象，但机器学习里最核心的一件事——**训练模型**——其实就一句话：  
> 在一个（可能非常非常高维的）空间里，沿着某个方向移动参数，让**损失函数**下降。

你在课程里会反复见到这些词：**梯度**、**链式法则**、**反向传播**、**雅可比矩阵**、**海森矩阵**。它们不是“数学家的装饰品”，而是模型能不能学会、训练快不快、会不会数值爆炸的底层原因。

### 基本概念

1. **多变量函数**（*multivariable function*）：输入是多个变量，输出是一个数或一个向量的函数。  
   - **标量值函数**： $\small{f:\mathbb{R}^n\rightarrow\mathbb{R}}$ ，例如损失函数 $\small{L(\theta)}$ 。  
   - **向量值函数**： $\small{\mathbf{g}:\mathbb{R}^n\rightarrow\mathbb{R}^m}$ ，例如神经网络的中间层输出 $\small{\mathbf{h}(\theta)}$ 。

2. **点**与**邻域**：在 $\small{\mathbb{R}^n}$ 中，一个参数向量 $\small{\mathbf{x}=[x_1,\dots,x_n]^T}$ 就是一个点。所谓“在某点附近”，就是在它的**邻域**里做微小扰动。

3. **可微性**：你可以把它直觉理解为：函数在某点附近“足够光滑”，用线性函数就能很好地近似它（后面会用到**泰勒展开**）。

> **说明**：机器学习里，我们经常默认损失函数在大部分区域是可微的；即使像 ReLU 这种在 0 点不可导的函数，工程上也可以用“次梯度”或约定处理，不影响训练的主流程。

### 偏导数：先学会“盯住一个变量不放”

**偏导数**（*partial derivative*）的意思是：当我们研究 $\small{f(x,y)}$ 对 $\small{x}$ 的变化时，先把 $\small{y}$ 当成常数。

对二元函数 $\small{f(x,y)}$，**偏导数**定义为：

$$
\frac{\partial f}{\partial x}(x,y) = \lim_{h\to 0}\frac{f(x+h,y)-f(x,y)}{h} \tag{1}
$$

同理：

$$
\frac{\partial f}{\partial y}(x,y) = \lim_{h\to 0}\frac{f(x,y+h)-f(x,y)}{h} \tag{2}
$$

如果 $\small{f}$ 有 $\small{n}$ 个变量，就有 $\small{n}$ 个偏导数 $\small{\frac{\partial f}{\partial x_i}}$。

举个例子，令 $\small{f(x,y)=x^2y+3y}$，则  
- 对 $\small{x}$ 求偏导（把 $\small{y}$ 当常数）：

$$
\frac{\partial f}{\partial x}=2xy \tag{3}
$$

- 对 $\small{y}$ 求偏导（把 $\small{x}$ 当常数）：

$$
\frac{\partial f}{\partial y}=x^2+3 \tag{4}
$$

### 梯度：多变量函数的“最陡方向”

当 $\small{f:\mathbb{R}^n\rightarrow\mathbb{R}}$ 时，我们把所有偏导数组成一个向量，得到**梯度**（*gradient*）：

$$
\nabla f(\mathbf{x}) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\\\ \frac{\partial f}{\partial x_2} \\\\ \vdots \\\\ \frac{\partial f}{\partial x_n} \end{bmatrix} \tag{5}
$$

**梯度**有一个极其重要的几何意义：  在点 $\small{\mathbf{x}}$ 处， $\small{\nabla f(\mathbf{x})}$ 指向函数值增长最快的方向； $\small{-\nabla f(\mathbf{x})}$ 指向下降最快的方向。

这就是为什么 **梯度下降**（*gradient descent*）会写成：

$$
\mathbf{x}_{t+1}=\mathbf{x}_{t}-\eta \nabla f(\mathbf{x}_t) \tag{6}
$$

其中，$\small{\eta>0}$ 代表**学习率**（*learning rate*），它控制每一步走多远。

### 方向导数：沿着任意方向看“变化率”

有时我们不想只看坐标轴方向（偏导数），而是想沿任意方向 $\small{\mathbf{u}}$（通常取单位向量）看变化率，这就是**方向导数**（*directional derivative*）。

**方向导数**定义为：

$$
D_{\mathbf{u}}f(\mathbf{x}) = \lim_{h\to 0}\frac{f(\mathbf{x}+h\mathbf{u})-f(\mathbf{x})}{h} \tag{7}
$$

如果 $\small{f}$ 在 $\small{\mathbf{x}}$ 可微，则有一个非常漂亮的结论：

$$
D_{\mathbf{u}}f(\mathbf{x}) = \nabla f(\mathbf{x})^T\mathbf{u} \tag{8}
$$

这意味着当 $\small{\mathbf{u}}$ 与 $\small{\nabla f}$ 同方向时变化最大，当 $\small{\mathbf{u}}$ 与 $\small{\nabla f}$ 垂直时变化为 0（沿等高线走）。
### 链式法则：反向传播的“数学骨架”

机器学习里最常见的结构是“函数套函数”。比如：

$$
y=f(g(x)) \tag{9}
$$

一元情形的 **链式法则**（*chain rule*）是：

$$
\frac{dy}{dx} = f'(g(x))\cdot g'(x) \tag{10}
$$

多变量的链式法则更像“把导数当作线性映射来组合”。为了不把读者吓跑，我们先给一个常用且直观的版本：

若 $\small{z=f(x,y)}$ ，且 $\small{x=x(t),y=y(t)}$ ，则：

$$
\frac{dz}{dt} = \frac{\partial f}{\partial x}\frac{dx}{dt} + \frac{\partial f}{\partial y}\frac{dy}{dt} \tag{11}
$$

这就是你在反向传播里看到“梯度一层层往回乘”的来源。

### 雅可比矩阵：向量值函数的“一阶导”

当输出是向量时： $\small{\mathbf{g}:\mathbb{R}^n\rightarrow\mathbb{R}^m}$ ，我们用 **雅可比矩阵**（*Jacobian matrix*）记录所有一阶偏导：

$$
\mathbf{J}_{\mathbf{g}}(\mathbf{x}) = \begin{bmatrix} \frac{\partial g_1}{\partial x_1} & \cdots & \frac{\partial g_1}{\partial x_n} \\\\ \vdots & \ddots & \vdots \\\\ \frac{\partial g_m}{\partial x_1} & \cdots & \frac{\partial g_m}{\partial x_n} \end{bmatrix} \tag{12}
$$

你可以把它理解为：在 $\small{\mathbf{x}}$ 附近，用一个线性变换近似 $\small{\mathbf{g}}$ ：

$$
\mathbf{g}(\mathbf{x}+\Delta\mathbf{x}) \approx \mathbf{g}(\mathbf{x}) + \mathbf{J}_{\mathbf{g}}(\mathbf{x})\,\Delta\mathbf{x} \tag{13}
$$

> **说明**：在深度学习框架里，自动求导本质上就是在高效地构建（或隐式使用）这些雅可比结构，而不是在纸上真的把大矩阵写出来。

### 海森矩阵：二阶信息与“曲率”

当 $\small{f:\mathbb{R}^n\rightarrow\mathbb{R}}$ 时，二阶偏导组成的矩阵叫**海森矩阵**（*Hessian matrix*）：

$$
\mathbf{H}_f(\mathbf{x}) = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} & \cdots & \frac{\partial^2 f}{\partial x_1\partial x_n} \\\\ \vdots & \ddots & \vdots \\\\ \frac{\partial^2 f}{\partial x_n\partial x_1} & \cdots & \frac{\partial^2 f}{\partial x_n^2} \end{bmatrix} \tag{14}
$$

直觉上，**梯度**告诉你“往哪走”，**海森矩阵**告诉你“地形陡不陡、弯不弯”，也就是**曲率**。这也是为什么二阶方法（如牛顿法）更“聪明”，因为它不仅看方向，还看曲率。
### 二阶泰勒展开：把函数在局部“拍扁”

在一元情况下，我们用泰勒展开做近似；多变量也一样。对可二次可微的 $\small{f}$ ，在 $\small{\mathbf{x}}$ 附近：

$$
f(\mathbf{x}+\Delta\mathbf{x}) \approx f(\mathbf{x}) + \nabla f(\mathbf{x})^T\Delta\mathbf{x} + \frac{1}{2}\Delta\mathbf{x}^T\mathbf{H}_f(\mathbf{x})\Delta\mathbf{x} \tag{15}
$$

这条式子非常关键，因为它让很多优化算法“有理有据”。

### 一个机器学习里随处可见的例子：最小二乘的梯度

设线性回归的预测为：

$$
\hat{y}=\mathbf{w}^T\mathbf{x}+b \tag{16}
$$

对单个样本 $\small{(\mathbf{x},y)}$ ，常用的平方损失：

$$
\ell(\mathbf{w},b)=\frac{1}{2}\left(\mathbf{w}^T\mathbf{x}+b-y\right)^2 \tag{17}
$$

令 $\small{e=\mathbf{w}^T\mathbf{x}+b-y}$ ，则：

$$
\nabla_{\mathbf{w}}\ell = e\,\mathbf{x} \tag{18}
$$

$$
\frac{\partial \ell}{\partial b} = e \tag{19}
$$

> **说明**：这里写 $\frac{1}{2}$ 的好处是求导后前面的 2 会抵消，让梯度更“干净”。这属于工程上常见的“让公式更顺手”的小技巧。

### 你应该带走的“直觉清单”

- **偏导数**：盯住一个变量看变化。  
- **梯度**：把所有偏导打包成一个向量，告诉你“最陡上升方向”。  
- **方向导数**：沿任意方向看变化率，本质是梯度与方向的点积。  
- **链式法则**：函数套函数时，导数要“层层相乘/相加”，反向传播就靠它。  
- **雅可比矩阵**：向量值函数的一阶导，是局部线性近似的核心。  
- **海森矩阵**：二阶导描述曲率，决定“走一步会不会过头”。  
- **二阶泰勒展开**：把局部函数近似成“线性+二次项”，优化算法的理论底座之一。

如果你能把“训练模型”看成“在高维地形上找最低点”，那么多变量微积分就不再是一堆符号，而是一张非常实用的导航图。
